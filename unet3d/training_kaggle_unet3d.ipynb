{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-10-15T14:44:43.681377Z","iopub.status.busy":"2024-10-15T14:44:43.681005Z","iopub.status.idle":"2024-10-15T14:44:47.715134Z","shell.execute_reply":"2024-10-15T14:44:47.714094Z","shell.execute_reply.started":"2024-10-15T14:44:43.681334Z"},"id":"8Uu_kLnmyv9-","trusted":true},"outputs":[],"source":["import collections.abc\n","import re\n","\n","import torch\n","from torch.nn import functional as F\n","\n","\n","def pad_tensor(x, l, pad_value=0):\n","    padlen = l - x.shape[0]\n","    pad = [0 for _ in range(2 * len(x.shape[1:]))] + [0, padlen]\n","    return F.pad(x, pad=pad, value=pad_value)\n","\n","\n","np_str_obj_array_pattern = re.compile(r\"[SaUO]\")\n","\n","\n","def pad_collate(batch, pad_value=0):\n","    # Utility function to be used as collate_fn for the PyTorch dataloader\n","    # to handle sequences of varying length.\n","    # Sequences are padded with zeros by default.\n","    #\n","    # Modified default_collate from the official pytorch repo\n","    # https://github.com/pytorch/pytorch/blob/master/torch/utils/data/_utils/collate.py\n","    elem = batch[0]\n","    elem_type = type(elem)\n","    if isinstance(elem, torch.Tensor):\n","        out = None\n","        if len(elem.shape) > 0:\n","            sizes = [e.shape[0] for e in batch]\n","            m = max(sizes)\n","            if not all(s == m for s in sizes):\n","                # pad tensors which have a temporal dimension\n","                batch = [pad_tensor(e, m, pad_value=pad_value) for e in batch]\n","        if torch.utils.data.get_worker_info() is not None:\n","            # If we're in a background process, concatenate directly into a\n","            # shared memory tensor to avoid an extra copy\n","            numel = sum([x.numel() for x in batch])\n","            storage = elem.storage()._new_shared(numel)\n","            out = elem.new(storage)\n","        return torch.stack(batch, 0, out=out)\n","    elif (\n","        elem_type.__module__ == \"numpy\"\n","        and elem_type.__name__ != \"str_\"\n","        and elem_type.__name__ != \"string_\"\n","    ):\n","        if elem_type.__name__ == \"ndarray\" or elem_type.__name__ == \"memmap\":\n","            # array of string classes and object\n","            if np_str_obj_array_pattern.search(elem.dtype.str) is not None:\n","                raise TypeError(\"Format not managed : {}\".format(elem.dtype))\n","\n","            return pad_collate([torch.as_tensor(b) for b in batch])\n","        elif elem.shape == ():  # scalars\n","            return torch.as_tensor(batch)\n","\n","    elif isinstance(elem, collections.abc.Mapping):\n","        return {key: pad_collate([d[key] for d in batch]) for key in elem}\n","\n","    elif isinstance(elem, tuple) and hasattr(elem, \"_fields\"):  # namedtuple\n","        return elem_type(*(pad_collate(samples) for samples in zip(*batch)))\n","\n","    elif isinstance(elem, collections.abc.Sequence):\n","        # check to make sure that the elements in batch have consistent size\n","        it = iter(batch)\n","        elem_size = len(next(it))\n","        if not all(len(elem) == elem_size for elem in it):\n","            raise RuntimeError(\"each element in list of batch should be of equal size\")\n","        transposed = zip(*batch)\n","        return [pad_collate(samples) for samples in transposed]\n","\n","    raise TypeError(\"Format not managed : {}\".format(elem_type))\n"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-10-15T14:44:47.717597Z","iopub.status.busy":"2024-10-15T14:44:47.717052Z","iopub.status.idle":"2024-10-15T14:44:48.416602Z","shell.execute_reply":"2024-10-15T14:44:48.415701Z","shell.execute_reply.started":"2024-10-15T14:44:47.717546Z"},"id":"A_DxM0Atyxxo","trusted":true},"outputs":[],"source":["\"\"\"\n","Baseline Pytorch Dataset\n","\"\"\"\n","\n","import os\n","from pathlib import Path\n","\n","import geopandas as gpd\n","import numpy as np\n","import torch\n","\n","\n","class BaselineDataset(torch.utils.data.Dataset):\n","    def __init__(self, folder: Path):\n","        super(BaselineDataset, self).__init__()\n","        self.folder = folder\n","\n","        # Get metadata\n","        print(\"Reading patch metadata ...\")\n","        self.meta_patch = gpd.read_file(os.path.join(folder, \"metadata.geojson\"))\n","        self.meta_patch.index = self.meta_patch[\"ID\"].astype(int)\n","        self.meta_patch.sort_index(inplace=True)\n","        print(\"Done.\")\n","\n","        self.len = self.meta_patch.shape[0]\n","        self.id_patches = self.meta_patch.index\n","        print(\"Dataset ready.\")\n","\n","    def __len__(self) -> int:\n","        return self.len\n","\n","    def __getitem__(self, item: int) -> tuple[dict[str, torch.Tensor], torch.Tensor]:\n","        id_patch = self.id_patches[item]\n","\n","        # Open and prepare satellite data into T x C x H x W arrays\n","        path_patch = os.path.join(self.folder, \"DATA_S2\", \"S2_{}.npy\".format(id_patch))\n","        data = np.load(path_patch).astype(np.float32)\n","        data = {\"S2\": torch.from_numpy(data)}\n","\n","        # If you have other modalities, add them as fields of the `data` dict ...\n","        # data[\"radar\"] = ...\n","\n","        # Open and prepare targets\n","        target = np.load(\n","            os.path.join(self.folder, \"ANNOTATIONS\", \"TARGET_{}.npy\".format(id_patch))\n","        )\n","        target = torch.from_numpy(target[0].astype(int))\n","\n","        return data, target\n"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-10-15T14:44:48.418878Z","iopub.status.busy":"2024-10-15T14:44:48.417992Z","iopub.status.idle":"2024-10-15T14:44:48.428394Z","shell.execute_reply":"2024-10-15T14:44:48.427207Z","shell.execute_reply.started":"2024-10-15T14:44:48.418826Z"},"id":"CgkgruPNSGvK","trusted":true},"outputs":[],"source":["import torch.nn as nn\n","\n","\n","class SimpleSegmentationModel(nn.Module):\n","    def __init__(self, input_channels: int, nb_classes: int):\n","        super(SimpleSegmentationModel, self).__init__()\n","\n","        # A very basic architecture: Encoder + Decoder\n","        self.encoder = nn.Sequential(\n","            nn.Conv2d(input_channels, 32, kernel_size=3, padding=1),\n","            nn.ReLU(),\n","            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n","            nn.ReLU(),\n","            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n","            nn.ReLU(),\n","        )\n","        self.decoder = nn.Sequential(\n","            nn.Conv2d(128, 64, kernel_size=3, padding=1),\n","            nn.ReLU(),\n","            nn.Conv2d(64, nb_classes, kernel_size=3, padding=1),\n","        )\n","\n","    def forward(self, x):\n","        # Input x shape: (B, Channels, H, W)\n","        x = self.encoder(x)\n","        x = self.decoder(x)\n","        # Output x shape: (B, Classes, H, W)\n","        return x\n"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-10-15T14:44:48.431246Z","iopub.status.busy":"2024-10-15T14:44:48.430876Z","iopub.status.idle":"2024-10-15T14:44:49.041180Z","shell.execute_reply":"2024-10-15T14:44:49.040127Z","shell.execute_reply.started":"2024-10-15T14:44:48.431199Z"},"id":"Lcvc0BkkSJLl","trusted":true},"outputs":[],"source":["from pathlib import Path\n","from sklearn.metrics import jaccard_score\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from tqdm import tqdm\n","\n","def print_iou_per_class(\n","    targets: torch.Tensor,\n","    preds: torch.Tensor,\n","    nb_classes: int,\n",") -> None:\n","    \"\"\"\n","    Compute IoU between predictions and targets, for each class.\n","\n","    Args:\n","        targets (torch.Tensor): Ground truth of shape (B, H, W).\n","        preds (torch.Tensor): Model predictions of shape (B, nb_classes, H, W).\n","        nb_classes (int): Number of classes in the segmentation task.\n","    \"\"\"\n","\n","    # Compute IoU for each class\n","    # Note: I use this for loop to iterate also on classes not in the demo batch\n","\n","    iou_per_class = []\n","    for class_id in range(nb_classes):\n","        iou = jaccard_score(\n","            targets == class_id,\n","            preds == class_id,\n","            average=\"binary\",\n","            zero_division=0,\n","        )\n","        iou_per_class.append(iou)\n","\n","    for class_id, iou in enumerate(iou_per_class):\n","        print(\n","            \"class {} - IoU: {:.4f} - targets: {} - preds: {}\".format(\n","                class_id, iou, (targets == class_id).sum(), (preds == class_id).sum()\n","            )\n","        )\n","\n","\n","def print_mean_iou(targets: torch.Tensor, preds: torch.Tensor) -> None:\n","    \"\"\"\n","    Compute mean IoU between predictions and targets.\n","\n","    Args:\n","        targets (torch.Tensor): Ground truth of shape (B, H, W).\n","        preds (torch.Tensor): Model predictions of shape (B, nb_classes, H, W).\n","    \"\"\"\n","\n","    mean_iou = jaccard_score(targets, preds, average=\"macro\")\n","    print(f\"meanIOU (over existing classes in targets): {mean_iou:.4f}\")\n","\n","\n","def train_model(\n","    data_folder: Path,\n","    nb_classes: int,\n","    input_channels: int,\n","    num_epochs: int = 10,\n","    batch_size: int = 4,\n","    learning_rate: float = 1e-3,\n","    device: str = \"cpu\",\n","    verbose: bool = False,\n",") -> SimpleSegmentationModel:\n","    \"\"\"\n","    Training pipeline.\n","    \"\"\"\n","    # Create data loader\n","    dataset = BaselineDataset(data_folder)\n","    dataloader = torch.utils.data.DataLoader(\n","        dataset, batch_size=batch_size, collate_fn=pad_collate, shuffle=True\n","    )\n","\n","    # Initialize the model, loss function, and optimizer\n","    model = SimpleSegmentationModel(input_channels, nb_classes)\n","    criterion = nn.CrossEntropyLoss()\n","    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","\n","    # Move the model to the appropriate device (GPU if available)\n","    device = torch.device(device)\n","    model.to(device)\n","\n","    # Training loop\n","    for epoch in range(num_epochs):\n","        model.train()  # Set the model to training mode\n","        running_loss = 0.0\n","\n","        for i, (inputs, targets) in tqdm(enumerate(dataloader), total=len(dataloader)):\n","            # Move data to device\n","            inputs[\"S2\"] = inputs[\"S2\"].to(device)  # Satellite data\n","            targets = targets.to(device)\n","\n","            # Zero the parameter gradients\n","            optimizer.zero_grad()\n","\n","            # Forward pass\n","            outputs = model(torch.median(inputs[\"S2\"],1).values)  # only use the 10th image\n","\n","            # Loss computation\n","            loss = criterion(outputs, targets)\n","\n","            # Backward pass and optimization\n","            loss.backward()\n","            optimizer.step()\n","            running_loss += loss.item()\n","\n","            # Get the predicted class per pixel (B, H, W)\n","            preds = torch.argmax(outputs, dim=1)\n","\n","            # Move data from GPU/Metal to CPU\n","            targets = targets.cpu().numpy().flatten()\n","            preds = preds.cpu().numpy().flatten()\n","\n","            if verbose:\n","                # Print IOU for debugging\n","                print_iou_per_class(targets, preds, nb_classes)\n","                print_mean_iou(targets, preds)\n","\n","        # Print the loss for this epoch\n","        epoch_loss = running_loss / len(dataloader)\n","        print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {epoch_loss:.4f}\")\n","\n","    print(\"Training complete.\")\n","    return model\n"]},{"cell_type":"markdown","metadata":{"id":"wLuu64_XSWK_"},"source":["# New model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ku2hif3pSWAR"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-10-15T15:02:07.448314Z","iopub.status.busy":"2024-10-15T15:02:07.447885Z","iopub.status.idle":"2024-10-15T15:02:07.553239Z","shell.execute_reply":"2024-10-15T15:02:07.552294Z","shell.execute_reply.started":"2024-10-15T15:02:07.448273Z"},"id":"QI2zLiGu6a18","trusted":true},"outputs":[],"source":["from enum import Enum, IntEnum\n","from typing import Optional, Union\n","\n","import torch\n","from torch import nn\n","\n","\n","class ActivationFunction(str, Enum):\n","    RELU: str = \"relu\"\n","    LEAKY: str = \"leaky\"\n","    ELU: str = \"elu\"\n","\n","\n","class NormalizationLayer(str, Enum):\n","    BATCH: str = \"batch\"\n","    INSTANCE: str = \"instance\"\n","\n","\n","class Dimensions(IntEnum):\n","    TWO: int = 2\n","    THREE: int = 3\n","\n","\n","class ConvMode(str, Enum):\n","    SAME: str = \"same\"\n","    VALID: str = \"valid\"\n","\n","\n","class UpMode(str, Enum):\n","    TRANSPOSED: str = \"transposed\"\n","    NEAREST: str = \"nearest\"\n","    LINEAR: str = \"linear\"\n","    BILINEAR: str = \"bilinear\"\n","    BICUBIC: str = \"bicubic\"\n","    TRILINEAR: str = \"trilinear\"\n","\n","\n","@torch.jit.script\n","def autocrop(encoder_layer: torch.Tensor, decoder_layer: torch.Tensor):\n","    \"\"\"\n","    Center-crops the encoder_layer to the size of the decoder_layer,\n","    so that merging (concatenation) between levels/blocks is possible.\n","    This is only necessary for input sizes != 2**n for 'same' padding and always required for 'valid' padding.\n","    \"\"\"\n","    if encoder_layer.shape[2:] != decoder_layer.shape[2:]:\n","        ds = encoder_layer.shape[2:]\n","        es = decoder_layer.shape[2:]\n","        assert ds[0] >= es[0]\n","        assert ds[1] >= es[1]\n","        if encoder_layer.dim() == 4:  # 2D\n","            encoder_layer = encoder_layer[\n","                :,\n","                :,\n","                ((ds[0] - es[0]) // 2) : ((ds[0] + es[0]) // 2),\n","                ((ds[1] - es[1]) // 2) : ((ds[1] + es[1]) // 2),\n","            ]\n","        elif encoder_layer.dim() == 5:  # 3D\n","            assert ds[2] >= es[2]\n","            encoder_layer = encoder_layer[\n","                :,\n","                :,\n","                ((ds[0] - es[0]) // 2) : ((ds[0] + es[0]) // 2),\n","                ((ds[1] - es[1]) // 2) : ((ds[1] + es[1]) // 2),\n","                ((ds[2] - es[2]) // 2) : ((ds[2] + es[2]) // 2),\n","            ]\n","    return encoder_layer, decoder_layer\n","\n","\n","def conv_layer(dim: int) -> Union[nn.Conv2d, nn.Conv3d]:\n","    conv_layers: dict = {Dimensions.TWO: nn.Conv2d, Dimensions.THREE: nn.Conv3d}\n","    return conv_layers[dim]\n","\n","\n","def get_conv_layer(\n","    in_channels: int,\n","    out_channels: int,\n","    kernel_size: int = 3,\n","    stride: int = 1,\n","    padding: int = 1,\n","    bias: bool = True,\n","    dim: int = Dimensions.TWO,\n",") -> Union[nn.Conv2d, nn.Conv3d]:\n","    layer: Union[nn.Conv2d, nn.Conv3d] = conv_layer(dim=dim)\n","    return layer(\n","        in_channels=in_channels,\n","        out_channels=out_channels,\n","        kernel_size=kernel_size,\n","        stride=stride,\n","        padding=padding,\n","        bias=bias,\n","    )\n","\n","\n","def conv_transpose_layer(dim: int) -> Union[nn.ConvTranspose2d, nn.ConvTranspose3d]:\n","    conv_transpose_layers: dict = {\n","        Dimensions.TWO: nn.ConvTranspose2d,\n","        Dimensions.THREE: nn.ConvTranspose3d,\n","    }\n","\n","    return conv_transpose_layers[dim]\n","\n","\n","def get_up_layer(\n","    in_channels: int,\n","    out_channels: int,\n","    kernel_size: int = 2,\n","    stride: int = 2,\n","    dim: int = Dimensions.TWO,\n","    up_mode: str = UpMode.TRANSPOSED,\n",") -> Union[Union[nn.ConvTranspose2d, nn.ConvTranspose3d], nn.Upsample]:\n","    if up_mode == UpMode.TRANSPOSED:\n","        return conv_transpose_layer(dim=dim)(\n","            in_channels=in_channels,\n","            out_channels=out_channels,\n","            kernel_size=kernel_size,\n","            stride=stride,\n","        )\n","    else:\n","        return nn.Upsample(scale_factor=2.0, mode=up_mode)\n","\n","\n","def maxpool_layer(dim: int) -> Union[nn.MaxPool2d, nn.MaxPool3d]:\n","    maxpool_layers: dict = {\n","        Dimensions.TWO: nn.MaxPool2d,\n","        Dimensions.THREE: nn.MaxPool3d,\n","    }\n","    return maxpool_layers[dim]\n","\n","\n","def get_maxpool_layer(\n","    kernel_size: int = 2, stride: int = 2, padding: int = 0, dim: int = Dimensions.TWO\n",") -> Union[nn.MaxPool2d, nn.MaxPool3d]:\n","    layer = maxpool_layer(dim=dim)\n","    return layer(kernel_size=kernel_size, stride=stride, padding=padding)\n","\n","\n","def get_activation_layer(activation: str) -> Union[nn.ReLU, nn.LeakyReLU, nn.ELU]:\n","    activation_functions: dict = {\n","        ActivationFunction.RELU: nn.ReLU(),\n","        ActivationFunction.LEAKY: nn.LeakyReLU(negative_slope=0.1),\n","        ActivationFunction.ELU: nn.ELU(),\n","    }\n","\n","    return activation_functions[activation]\n","\n","\n","def get_normalization_layer(\n","    normalization: str, num_channels: int, dim: int\n",") -> Union[\n","    Union[nn.BatchNorm2d, nn.BatchNorm3d],\n","    Union[nn.InstanceNorm2d, nn.InstanceNorm3d],\n","]:\n","    normalization_layers: dict = {\n","        Dimensions.TWO: {\n","            NormalizationLayer.BATCH: nn.BatchNorm2d(num_channels),\n","            NormalizationLayer.INSTANCE: nn.InstanceNorm2d(num_channels),\n","        },\n","        Dimensions.THREE: {\n","            NormalizationLayer.BATCH: nn.BatchNorm3d(num_channels),\n","            NormalizationLayer.INSTANCE: nn.InstanceNorm3d(num_channels),\n","        },\n","    }\n","\n","    return normalization_layers[dim][normalization]\n","\n","\n","class Concatenate(nn.Module):\n","    def __init__(self):\n","        super(Concatenate, self).__init__()\n","\n","    def forward(self, layer_1, layer_2):\n","        x = torch.cat((layer_1, layer_2), 1)\n","\n","        return x\n","\n","\n","class DownBlock(nn.Module):\n","    \"\"\"\n","    A helper Module that performs 2 Convolutions and 1 MaxPool.\n","    An activation follows each convolution.\n","    A normalization layer follows each convolution.\n","    \"\"\"\n","\n","    def __init__(\n","        self,\n","        in_channels: int,\n","        out_channels: int,\n","        pooling: bool = True,\n","        activation: str = ActivationFunction.RELU,\n","        normalization: Optional[str] = None,\n","        dim: int = Dimensions.TWO,\n","        conv_mode: str = ConvMode.SAME,\n","    ):\n","        super().__init__()\n","\n","        conv_modes: dict = {ConvMode.SAME: 1, ConvMode.VALID: 0}\n","\n","        self.in_channels = in_channels\n","        self.out_channels = out_channels\n","        self.pooling = pooling\n","        self.normalization = normalization\n","        self.padding = conv_modes[conv_mode]\n","        self.dim = dim\n","        self.activation = activation\n","\n","        # conv layers\n","        self.conv1 = get_conv_layer(\n","            in_channels=self.in_channels,\n","            out_channels=self.out_channels,\n","            kernel_size=3,\n","            stride=1,\n","            padding=self.padding,\n","            bias=True,\n","            dim=self.dim,\n","        )\n","        self.conv2 = get_conv_layer(\n","            in_channels=self.out_channels,\n","            out_channels=self.out_channels,\n","            kernel_size=3,\n","            stride=1,\n","            padding=self.padding,\n","            bias=True,\n","            dim=self.dim,\n","        )\n","\n","        # pooling layer\n","        if self.pooling:\n","            self.pool = get_maxpool_layer(\n","                kernel_size=2, stride=2, padding=0, dim=self.dim\n","            )\n","\n","        # activation layers\n","        self.act1 = get_activation_layer(activation=self.activation)\n","        self.act2 = get_activation_layer(activation=self.activation)\n","\n","        # normalization layers\n","        if self.normalization:\n","            self.norm1 = get_normalization_layer(\n","                normalization=self.normalization,\n","                num_channels=self.out_channels,\n","                dim=self.dim,\n","            )\n","            self.norm2 = get_normalization_layer(\n","                normalization=self.normalization,\n","                num_channels=self.out_channels,\n","                dim=self.dim,\n","            )\n","\n","    def forward(self, x):\n","        y = self.conv1(x)  # convolution 1\n","        y = self.act1(y)  # activation 1\n","        if self.normalization:\n","            y = self.norm1(y)  # normalization 1\n","        y = self.conv2(y)  # convolution 2\n","        y = self.act2(y)  # activation 2\n","        if self.normalization:\n","            y = self.norm2(y)  # normalization 2\n","\n","        before_pooling = y  # save the outputs before the pooling operation\n","        if self.pooling:\n","            y = self.pool(y)  # pooling\n","        return y, before_pooling\n","\n","\n","class UpBlock(nn.Module):\n","    \"\"\"\n","    A helper Module that performs 2 Convolutions and 1 UpConvolution/Upsample.\n","    An activation follows each convolution.\n","    A normalization layer follows each convolution.\n","    \"\"\"\n","\n","    def __init__(\n","        self,\n","        in_channels: int,\n","        out_channels: int,\n","        activation: str = ActivationFunction.RELU,\n","        normalization: Optional[str] = None,\n","        dim: int = Dimensions.TWO,\n","        conv_mode: str = ConvMode.SAME,\n","        up_mode: str = UpMode.TRANSPOSED,\n","    ):\n","        super().__init__()\n","\n","        conv_modes: dict = {ConvMode.SAME: 1, ConvMode.VALID: 0}\n","\n","        self.in_channels = in_channels\n","        self.out_channels = out_channels\n","        self.normalization = normalization\n","        self.padding = conv_modes[conv_mode]\n","        self.dim = dim\n","        self.activation = activation\n","\n","        self.up_mode = up_mode\n","\n","        # upconvolution/upsample layer\n","        self.up = get_up_layer(\n","            in_channels=self.in_channels,\n","            out_channels=self.out_channels,\n","            kernel_size=2,\n","            stride=2,\n","            dim=self.dim,\n","            up_mode=self.up_mode,\n","        )\n","\n","        # conv layers\n","        self.conv0 = get_conv_layer(\n","            in_channels=self.in_channels,\n","            out_channels=self.out_channels,\n","            kernel_size=1,\n","            stride=1,\n","            padding=0,\n","            bias=True,\n","            dim=self.dim,\n","        )\n","        self.conv1 = get_conv_layer(\n","            in_channels=2 * self.out_channels,\n","            out_channels=self.out_channels,\n","            kernel_size=3,\n","            stride=1,\n","            padding=self.padding,\n","            bias=True,\n","            dim=self.dim,\n","        )\n","        self.conv2 = get_conv_layer(\n","            in_channels=self.out_channels,\n","            out_channels=self.out_channels,\n","            kernel_size=3,\n","            stride=1,\n","            padding=self.padding,\n","            bias=True,\n","            dim=self.dim,\n","        )\n","\n","        # activation layers\n","        self.act0 = get_activation_layer(self.activation)\n","        self.act1 = get_activation_layer(self.activation)\n","        self.act2 = get_activation_layer(self.activation)\n","\n","        # normalization layers\n","        if self.normalization:\n","            self.norm0 = get_normalization_layer(\n","                normalization=self.normalization,\n","                num_channels=self.out_channels,\n","                dim=self.dim,\n","            )\n","            self.norm1 = get_normalization_layer(\n","                normalization=self.normalization,\n","                num_channels=self.out_channels,\n","                dim=self.dim,\n","            )\n","            self.norm2 = get_normalization_layer(\n","                normalization=self.normalization,\n","                num_channels=self.out_channels,\n","                dim=self.dim,\n","            )\n","\n","        # concatenate layer\n","        self.concat = Concatenate()\n","\n","    def forward(self, encoder_layer, decoder_layer):\n","        \"\"\"\n","        Forward pass\n","        encoder_layer: Tensor from the encoder pathway\n","        decoder_layer: Tensor from the decoder pathway (to be up'd)\n","        \"\"\"\n","        up_layer = self.up(decoder_layer)  # up-convolution/up-sampling\n","        cropped_encoder_layer, dec_layer = autocrop(encoder_layer, up_layer)  # cropping\n","\n","        if self.up_mode != UpMode.TRANSPOSED:\n","            # We need to reduce the channel dimension with a conv layer\n","            up_layer = self.conv0(up_layer)  # convolution 0\n","        up_layer = self.act0(up_layer)  # activation 0\n","        if self.normalization:\n","            up_layer = self.norm0(up_layer)  # normalization 0\n","\n","        merged_layer = self.concat(up_layer, cropped_encoder_layer)  # concatenation\n","        y = self.conv1(merged_layer)  # convolution 1\n","        y = self.act1(y)  # activation 1\n","        if self.normalization:\n","            y = self.norm1(y)  # normalization 1\n","        y = self.conv2(y)  # convolution 2\n","        y = self.act2(y)  # acivation 2\n","        if self.normalization:\n","            y = self.norm2(y)  # normalization 2\n","        return y\n","\n","\n","class UNet(nn.Module):\n","    \"\"\"\n","    activation: 'relu', 'leaky', 'elu'\n","    normalization: 'batch', 'instance', 'group{group_size}'\n","    conv_mode: 'same', 'valid'\n","    dim: 2, 3\n","    up_mode: 'transposed', 'nearest', 'linear', 'bilinear', 'bicubic', 'trilinear'\n","    \"\"\"\n","\n","    def __init__(\n","        self,\n","        in_channels: int = 1,\n","        out_channels: int = 2,\n","        n_blocks: int = 3,\n","        start_filters: int = 32,\n","        activation: str = ActivationFunction.RELU,\n","        normalization: str = NormalizationLayer.BATCH,\n","        conv_mode: str = ConvMode.SAME,\n","        dim: int = Dimensions.TWO,\n","        up_mode: str = UpMode.TRANSPOSED,\n","    ):\n","        super().__init__()\n","\n","        self.in_channels = in_channels\n","        self.out_channels = out_channels\n","        self.n_blocks = n_blocks\n","        self.start_filters = start_filters\n","        self.activation = activation\n","        self.normalization = normalization\n","        self.conv_mode = conv_mode\n","        self.dim = dim\n","        self.up_mode = up_mode\n","\n","        self.down_blocks = []\n","        self.up_blocks = []\n","\n","        # create encoder path\n","        for i in range(self.n_blocks):\n","            num_filters_in = self.in_channels if i == 0 else num_filters_out\n","            num_filters_out = self.start_filters * (2**i)\n","            pooling = True if i < self.n_blocks - 1 else False\n","\n","            down_block = DownBlock(\n","                in_channels=num_filters_in,\n","                out_channels=num_filters_out,\n","                pooling=pooling,\n","                activation=self.activation,\n","                normalization=self.normalization,\n","                conv_mode=self.conv_mode,\n","                dim=self.dim,\n","            )\n","\n","            self.down_blocks.append(down_block)\n","\n","        # create decoder path (requires only n_blocks-1 blocks)\n","        for i in range(n_blocks - 1):\n","            num_filters_in = num_filters_out\n","            num_filters_out = num_filters_in // 2\n","\n","            up_block = UpBlock(\n","                in_channels=num_filters_in,\n","                out_channels=num_filters_out,\n","                activation=self.activation,\n","                normalization=self.normalization,\n","                conv_mode=self.conv_mode,\n","                dim=self.dim,\n","                up_mode=self.up_mode,\n","            )\n","\n","            self.up_blocks.append(up_block)\n","\n","        # final convolution\n","        self.conv_final = get_conv_layer(\n","            num_filters_out,\n","            self.out_channels,\n","            kernel_size=1,\n","            stride=1,\n","            padding=0,\n","            bias=True,\n","            dim=self.dim,\n","        )\n","\n","        # add the list of modules to current module\n","        self.down_blocks = nn.ModuleList(self.down_blocks)\n","        self.up_blocks = nn.ModuleList(self.up_blocks)\n","\n","        # initialize the weights\n","        self.initialize_parameters()\n","\n","    @staticmethod\n","    def weight_init(module, method, **kwargs):\n","        if isinstance(\n","            module, (nn.Conv3d, nn.Conv2d, nn.ConvTranspose3d, nn.ConvTranspose2d)\n","        ):\n","            method(module.weight, **kwargs)  # weights\n","\n","    @staticmethod\n","    def bias_init(module, method, **kwargs):\n","        if isinstance(\n","            module, (nn.Conv3d, nn.Conv2d, nn.ConvTranspose3d, nn.ConvTranspose2d)\n","        ):\n","            method(module.bias, **kwargs)  # bias\n","\n","    def initialize_parameters(\n","        self, method_weights=nn.init.xavier_uniform_, method_bias=nn.init.zeros_\n","    ):\n","        for module in self.modules():\n","            self.weight_init(module, method_weights)  # initialize weights\n","            self.bias_init(module, method_bias)  # initialize bias\n","\n","    def forward(self, x: torch.tensor):\n","        encoder_output = []\n","\n","        # Encoder pathway\n","        x = torch.transpose(x,1,2)\n","        for module in self.down_blocks:\n","            x, before_pooling = module(x)\n","            encoder_output.append(before_pooling)\n","\n","        # Decoder pathway\n","        for i, module in enumerate(self.up_blocks):\n","            before_pool = encoder_output[-(i + 2)]\n","            x = module(before_pool, x)\n","\n","        x = self.conv_final(x)\n","\n","        return x\n","\n","    def __repr__(self):\n","        attributes = {\n","            attr_key: self.__dict__[attr_key]\n","            for attr_key in self.__dict__.keys()\n","            if \"_\" not in attr_key[0] and \"training\" not in attr_key\n","        }\n","        d = {self.__class__.__name__: attributes}\n","        return f\"{d}\"\n","\n","\n","# if __name__ == \"__main__\":\n","#     unet = UNet(\n","#         in_channels=1,\n","#         out_channels=2,\n","#         n_blocks=4,\n","#         start_filters=32,\n","#         activation=ActivationFunction.RELU,\n","#         normalization=NormalizationLayer.BATCH,\n","#         conv_mode=ConvMode.SAME,\n","#         dim=Dimensions.TWO,\n","#         up_mode=UpMode.TRANSPOSED,\n","#     )\n","#     from torchinfo import summary\n","\n","#     # [B, C, H, W]\n","#     summary = summary(model=unet, input_size=(1, 1, 512, 512), device=\"cpu\")"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-10-15T15:01:57.854031Z","iopub.status.busy":"2024-10-15T15:01:57.853631Z","iopub.status.idle":"2024-10-15T15:01:57.929392Z","shell.execute_reply":"2024-10-15T15:01:57.927868Z","shell.execute_reply.started":"2024-10-15T15:01:57.853990Z"},"id":"jyJkjcxFy9WC","trusted":true},"outputs":[{"ename":"NameError","evalue":"name 'UNet' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[11], line 66\u001b[0m\n\u001b[1;32m     53\u001b[0m     mean_iou \u001b[38;5;241m=\u001b[39m jaccard_score(targets, preds, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmacro\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeanIOU (over existing classes in targets): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmean_iou\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_model_2\u001b[39m(\n\u001b[1;32m     58\u001b[0m     data_folder: Path,\n\u001b[1;32m     59\u001b[0m     nb_classes: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m     60\u001b[0m     input_channels: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m     61\u001b[0m     num_epochs: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m,\n\u001b[1;32m     62\u001b[0m     batch_size: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m,\n\u001b[1;32m     63\u001b[0m     learning_rate: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1e-3\u001b[39m,\n\u001b[1;32m     64\u001b[0m     device: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     65\u001b[0m     verbose: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m---> 66\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[43mUNet\u001b[49m:\n\u001b[1;32m     67\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;124;03m    Training pipeline.\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;66;03m# Create data loader\u001b[39;00m\n","\u001b[0;31mNameError\u001b[0m: name 'UNet' is not defined"]}],"source":["from pathlib import Path\n","from sklearn.metrics import jaccard_score\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from tqdm import tqdm\n","\n","\n","def print_iou_per_class(\n","    targets: torch.Tensor,\n","    preds: torch.Tensor,\n","    nb_classes: int,\n",") -> None:\n","    \"\"\"\n","    Compute IoU between predictions and targets, for each class.\n","\n","    Args:\n","        targets (torch.Tensor): Ground truth of shape (B, H, W).\n","        preds (torch.Tensor): Model predictions of shape (B, nb_classes, H, W).\n","        nb_classes (int): Number of classes in the segmentation task.\n","    \"\"\"\n","\n","    # Compute IoU for each class\n","    # Note: I use this for loop to iterate also on classes not in the demo batch\n","\n","    iou_per_class = []\n","    for class_id in range(nb_classes):\n","        iou = jaccard_score(\n","            targets == class_id,\n","            preds == class_id,\n","            average=\"binary\",\n","            zero_division=0,\n","        )\n","        iou_per_class.append(iou)\n","\n","    for class_id, iou in enumerate(iou_per_class):\n","        print(\n","            \"class {} - IoU: {:.4f} - targets: {} - preds: {}\".format(\n","                class_id, iou, (targets == class_id).sum(), (preds == class_id).sum()\n","            )\n","        )\n","\n","\n","def print_mean_iou(targets: torch.Tensor, preds: torch.Tensor) -> None:\n","    \"\"\"\n","    Compute mean IoU between predictions and targets.\n","\n","    Args:\n","        targets (torch.Tensor): Ground truth of shape (B, H, W).\n","        preds (torch.Tensor): Model predictions of shape (B, nb_classes, H, W).\n","    \"\"\"\n","\n","    mean_iou = jaccard_score(targets, preds, average=\"macro\")\n","    print(f\"meanIOU (over existing classes in targets): {mean_iou:.4f}\")\n","\n","\n","def train_model_2(\n","    data_folder: Path,\n","    nb_classes: int,\n","    input_channels: int,\n","    num_epochs: int = 10,\n","    batch_size: int = 4,\n","    learning_rate: float = 1e-3,\n","    device: str = \"cpu\",\n","    verbose: bool = False,\n",") -> UNet:\n","    \"\"\"\n","    Training pipeline.\n","    \"\"\"\n","    # Create data loader\n","    dataset = BaselineDataset(data_folder)\n","    dataloader = torch.utils.data.DataLoader(\n","        dataset, batch_size=batch_size, collate_fn=pad_collate, shuffle=True\n","    )\n","\n","    # Initialize the model, loss function, and optimizer\n","    model = UNet( in_channels=input_channels,out_channels=nb_classes,dim=3)\n","    criterion = nn.CrossEntropyLoss()\n","    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","\n","    # Move the model to the appropriate device (GPU if available)\n","    device = torch.device(device)\n","    model.to(device)\n","\n","    # Training loop\n","    for epoch in range(num_epochs):\n","        model.train()  # Set the model to training mode\n","        running_loss = 0.0\n","\n","        for i, (inputs, targets) in tqdm(enumerate(dataloader), total=len(dataloader)):\n","            # Move data to device\n","            inputs[\"S2\"] = inputs[\"S2\"].to(device)  # Satellite data\n","            targets = targets.to(device)\n","\n","            # Zero the parameter gradients\n","            optimizer.zero_grad()\n","\n","            # Forward pass\n","\n","            outputs = model(inputs[\"S2\"]) \n","            outputs_median_time = torch.median(outputs,2).values\n","\n","            # Loss computation\n","            loss = criterion(outputs_median_time, targets)\n","\n","            # Backward pass and optimization\n","            loss.backward()\n","            optimizer.step()\n","            running_loss += loss.item()\n","\n","            # Get the predicted class per pixel (B, H, W)\n","            preds = torch.argmax(outputs_median_time, dim=1)\n","\n","            # Move data from GPU/Metal to CPU\n","            targets = targets.cpu().numpy().flatten()\n","            preds = preds.cpu().numpy().flatten()\n","\n","            if verbose:\n","                # Print IOU for debugging\n","                print_iou_per_class(targets, preds, nb_classes)\n","                print_mean_iou(targets, preds)\n","\n","        # Print the loss for this epoch\n","        epoch_loss = running_loss / len(dataloader)\n","        print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {epoch_loss:.4f}\")\n","\n","    print(\"Training complete.\")\n","    return model\n","\n","\n","# if __name__ == \"__main__\":\n","#     # Example usage:\n","#     model = train_model(\n","#         data_folder=Path(\n","#             \"/Users/louis.stefanuto.c/Documents/pastis-benchmark-mines2024/DATA/TRAIN/\"\n","#         ),\n","#         nb_classes=20,\n","#         input_channels=10,\n","#         num_epochs=100,\n","#         batch_size=32,\n","#         learning_rate=1e-3,\n","#         device=\"mps\",\n","#         verbose=True,\n","#     )\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":426},"id":"TLNQ3YTNqdiB","outputId":"64a4a895-e8f2-4771-dfcc-30ae5a17ac9f","trusted":true},"outputs":[],"source":["\n","DEVICE = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n","folder=Path('/kaggle/input/data-challenge-invent-mines-2024/DATA/DATA/TRAIN')\n","model = train_model_2(\n","            data_folder=folder,\n","            nb_classes= 20,\n","            input_channels=10,\n","            num_epochs = 10,\n","            batch_size= 5,\n","            learning_rate= 1e-3,\n","            device= DEVICE,\n","            verbose= True,\n","        )\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kuQzARRm1dVV","outputId":"2a23d42d-b91c-45d4-ac2b-5b417c30cb98"},"outputs":[],"source":["import gc\n","gc.collect()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"unlItSgy0wbi"},"outputs":[],"source":["torch.cuda.empty_cache()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S6G6b6nw1P-n","trusted":true},"outputs":[],"source":["torch.save(model.state_dict(), 'unet3d.pt')"]},{"cell_type":"code","execution_count":26,"metadata":{"execution":{"iopub.execute_input":"2024-10-15T15:10:39.619243Z","iopub.status.busy":"2024-10-15T15:10:39.618492Z","iopub.status.idle":"2024-10-15T15:11:50.098937Z","shell.execute_reply":"2024-10-15T15:11:50.097894Z","shell.execute_reply.started":"2024-10-15T15:10:39.619201Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Reading patch metadata ...\n","Done.\n","Dataset ready.\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 474/474 [01:10<00:00,  6.76it/s]\n"]}],"source":["import torch\n","from torch.utils.data import DataLoader\n","from torch import nn\n","model_file = 'unet3d.pt'\n","\n","\"\"\"\n","Baseline Pytorch Dataset\n","\"\"\"\n","\n","import os\n","from pathlib import Path\n","\n","import geopandas as gpd\n","import numpy as np\n","import torch\n","\n","\n","class OutputDataset(torch.utils.data.Dataset):\n","    def __init__(self, folder: Path):\n","        super(OutputDataset, self).__init__()\n","        self.folder = folder\n","\n","        # Get metadata\n","        print(\"Reading patch metadata ...\")\n","        self.meta_patch = gpd.read_file(os.path.join(folder, \"metadata.geojson\"))\n","        self.meta_patch.index = self.meta_patch[\"ID\"].astype(int)\n","        self.meta_patch.sort_index(inplace=True)\n","        print(\"Done.\")\n","\n","        self.len = self.meta_patch.shape[0]\n","        self.id_patches = self.meta_patch.index\n","        print(\"Dataset ready.\")\n","\n","    def __len__(self) -> int:\n","        return self.len\n","\n","    def __getitem__(self, item: int) -> tuple[dict[str, torch.Tensor], torch.Tensor]:\n","        id_patch = self.id_patches[item]\n","\n","        # Open and prepare satellite data into T x C x H x W arrays\n","        path_patch = os.path.join(self.folder, \"DATA_S2\", \"S2_{}.npy\".format(id_patch))\n","        data = np.load(path_patch).astype(np.float32)\n","        data = {\"S2\": torch.from_numpy(data)}\n","        data['ID']=id_patch\n","        # If you have other modalities, add them as fields of the `data` dict ...\n","        # data[\"radar\"] = ...\n","\n","        # Open and prepare targets\n","#         target = np.load(\n","#             os.path.join(self.folder, \"ANNOTATIONS\", \"TARGET_{}.npy\".format(id_patch))\n","#         )\n","#         target = torch.from_numpy(target[0].astype(int))\n","\n","        return data\n","\n","import numpy as np\n","\n","\n","def masks_to_str(predictions: np.ndarray) -> list[str]:\n","    \"\"\"\n","    Convert the\n","\n","    Args:\n","        predictions (np.ndarray): predictions as a 3D batch (B, H, W)\n","\n","    Returns:\n","        list[str]: a list of B strings, each string is a flattened stringified prediction mask\n","    \"\"\"\n","    return [\" \".join(f\"{x}\" for x in np.ravel(x)) for x in predictions]\n","\n","import pandas as pd \n","\n","def eval_model(\n","    data_folder: Path,\n","    model_file: str,\n","    batch_size: int = 1,\n","    device: str = \"cpu\",\n","\n",") -> None:\n","    \"\"\"\n","    Training pipeline.\n","    \"\"\"\n","    # Create data loader\n","    dataset = OutputDataset(data_folder)\n","    dataloader = torch.utils.data.DataLoader(\n","        dataset, batch_size=batch_size, collate_fn=pad_collate, shuffle=False\n","    )\n","\n","    # Load the saved model\n","    model = UNet(in_channels=10,out_channels=20,dim=3)\n","    model.load_state_dict(torch.load(model_file, weights_only=True))\n","    model.to(device)\n","\n","    # Set the model in evaluation mode\n","    model.eval()\n","\n","    # 3. Evaluate the Model on Test Samples\n","    # Disable gradient computation for evaluation\n","    res = []\n","    with torch.no_grad():\n","        for i, (inputs) in tqdm(enumerate(dataloader), total=len(dataloader)):\n","            # Move data to device\n","            inputs[\"S2\"] = inputs[\"S2\"].to(device)  # Satellite data\n","            patch_id = inputs['ID']\n","            # Forward pass through the model\n","            outputs = model(inputs['S2'])\n","            outputs_median_time = torch.median(outputs,2).values\n","            preds = torch.argmax(outputs_median_time, dim=1).cpu()\n","            preds_str = masks_to_str(preds)\n","            res.append([patch_id.item(),preds_str[0]])\n","    return pd.DataFrame(res,columns=['ID','MASKS'])\n","DEVICE = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n","preds = eval_model(data_folder='/kaggle/input/data-challenge-invent-mines-2024/DATA/DATA/TEST',model_file='/kaggle/input/unet3d/pytorch/default/1/unet3d.pt',device=DEVICE)\n","preds.to_csv('submissions_1_unet3d.csv',index=False)"]},{"cell_type":"code","execution_count":28,"metadata":{"execution":{"iopub.execute_input":"2024-10-15T15:11:50.111670Z","iopub.status.busy":"2024-10-15T15:11:50.111339Z","iopub.status.idle":"2024-10-15T15:11:50.131939Z","shell.execute_reply":"2024-10-15T15:11:50.130987Z","shell.execute_reply.started":"2024-10-15T15:11:50.111633Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>ID</th>\n","      <th>MASKS</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>20000</td>\n","      <td>0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>20001</td>\n","      <td>0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>20002</td>\n","      <td>19 19 19 19 19 19 19 19 19 9 9 9 9 9 9 9 9 9 9...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>20003</td>\n","      <td>19 19 19 19 19 19 19 19 19 19 19 19 19 19 19 1...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>20004</td>\n","      <td>0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>469</th>\n","      <td>20469</td>\n","      <td>0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 ...</td>\n","    </tr>\n","    <tr>\n","      <th>470</th>\n","      <td>20470</td>\n","      <td>19 1 19 19 19 19 19 19 19 19 19 19 19 19 1 1 1...</td>\n","    </tr>\n","    <tr>\n","      <th>471</th>\n","      <td>20471</td>\n","      <td>19 19 19 19 19 19 19 19 19 19 19 19 19 19 19 1...</td>\n","    </tr>\n","    <tr>\n","      <th>472</th>\n","      <td>20472</td>\n","      <td>19 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1...</td>\n","    </tr>\n","    <tr>\n","      <th>473</th>\n","      <td>20473</td>\n","      <td>0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>474 rows × 2 columns</p>\n","</div>"],"text/plain":["        ID                                              MASKS\n","0    20000  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...\n","1    20001  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...\n","2    20002  19 19 19 19 19 19 19 19 19 9 9 9 9 9 9 9 9 9 9...\n","3    20003  19 19 19 19 19 19 19 19 19 19 19 19 19 19 19 1...\n","4    20004  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...\n","..     ...                                                ...\n","469  20469  0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 ...\n","470  20470  19 1 19 19 19 19 19 19 19 19 19 19 19 19 1 1 1...\n","471  20471  19 19 19 19 19 19 19 19 19 19 19 19 19 19 19 1...\n","472  20472  19 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1...\n","473  20473  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...\n","\n","[474 rows x 2 columns]"]},"execution_count":28,"metadata":{},"output_type":"execute_result"}],"source":["preds"]},{"cell_type":"code","execution_count":29,"metadata":{"execution":{"iopub.execute_input":"2024-10-15T15:12:08.381238Z","iopub.status.busy":"2024-10-15T15:12:08.380777Z","iopub.status.idle":"2024-10-15T15:12:09.057793Z","shell.execute_reply":"2024-10-15T15:12:09.056863Z","shell.execute_reply.started":"2024-10-15T15:12:08.381197Z"},"trusted":true},"outputs":[],"source":["preds.to_csv('submissions_1_unet3d.csv',index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"databundleVersionId":9818909,"sourceId":86357,"sourceType":"competition"},{"isSourceIdPinned":true,"modelId":139159,"modelInstanceId":115932,"sourceId":136965,"sourceType":"modelInstanceVersion"}],"dockerImageVersionId":30787,"isGpuEnabled":true,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":4}
